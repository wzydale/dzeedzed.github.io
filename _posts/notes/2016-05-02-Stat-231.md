---
layout: post
title: STAT 231 - Statistics
date: 2016-05-02 18:56:00 -0400
categories: notes
--- 

    STAT 231 - Statistics
    Instructor: Surya Banerjee
    Location: DC 1350
    Time: Mondays and Wednesdays 2:30am - 3:50pm
    Tutorials: DC 1351 Mondays 5:30pm - 6:20pm
    Term: Spring 2016
    

## May 2, 2016 - Lecture 1 ##

Email: s22baner@uwaterloo.ca or suryabanerjee@gmail.com

Course Marking Scheme: 3 Tutorial Quizzes (TQ) and 2 Midterms

STAT 231 is the reverse of STAT 230. E.g. We toss a coin 100 times, we get 60 heads. In STAT 230, we calculate the probability of that happening (binomial distribution, 100 and 0.5, P(X=60)) with all the parameters given (P = 0.5 to get a head for a fair coin). In STAT 231, we try to find the parameters by experiments, and try to infer what we can say about the "fairness" of the coin.

MLB: Between 1901-1941, batters with BA .400+: 11. 1941 onwards: 0. Why? Use statistics to find out.

Kansas Weathermen: They are correct about whether it will rain the next day about 85% of the time. Good record, right? However, it only rains about 10% of the time in Kansas. So if I go to the weather broadcasting station and say "no rain tomorrow" and then go home, I will be statistically more accuarate that these meteorologists.

Baseball Pundits: Pundits are right about the outcome of baseball games about 48% of the time. Literally worse than a coin. 

In STAT 231, we learn how to test claims and check for correlation and causation. How can we make the leap from correlation to causation? (Recommended reading: Freakanomics)

<br />

## Introduction to Data Analysis ##

Data can be classified into two categories: numerical and categorical (non-numerical).

**Numerical Data** is either discrete or continuous. Discrete data (e.g. number of accidents) takes integer values, while continuous data (e.g. height, weight) measure non-integer values.

**Categorical Data** is either ordinal (if there's an underlying order, e.g. on a scale from 1 to 10, 1 being least satisfied and 10 being most satisfied) or non-ordinal (no order, e.g. nationality, favorite color).

**Transformation** {x<sub>1</sub>, x<sub>2</sub>,..., x<sub>n</sub>}  
y<sub>i</sub> = f(x<sub>i</sub>) is a transformation.


An affine transformation is a linear transformation: y<sub>i</sub> = ax<sub>i</sub> + b

**Coding** is the conversion of categorical data into numerical data.

<br />

Data Summaries
--------------

We want to extract important information about data.

1. Numerical: come up with numbers that represent different properties of the data set
2. Graphical: graph that tells us the shape of the data set

There are some properties of interest:

+ Centre of the data set -> **Central Tendency**

+ How volatile is the data set -> **Dispersion**

+ If the data set is symmetric -> **Measures of Symmetry**

+ How "fat" the tails of the data set are (i.e. the size of extrema) -> **Kurtosis**

## Measures of Central Tendency

Data set: {y<sub>1</sub>, y<sub>2</sub>,..., y<sub>n</sub>}

**Sample Mean:** 

Property: The sum of the deviations of the observations from the sample mean is 0.

**Geometric Mean**

**Harmonic Mean**

<br />

May 4, 2016 - Lecture 2
---

Lecture notes will be posted every weekend.  
Practice questions (with solutions) will be osted this week. First tutuorial quiz on May 17 (T), 2016 (11:25am).

**Outline**

+ measures of central tendency: medias, quartiles, mode  
+ measures of variability: range, IQR, variance

Sample -> Predictions about the population

Data Summaries
===

+ Centre: CENTRAL TENDENCY  
+ Variability: DISPERSION  
+ Tails are similar: SYMMETRY  
+ Frequency of Extreme Obersevations: KURTOSIS

Numerical and Graphical Summaries
===

{y<sub>1</sub>, y<sub>2</sub>, ... , y<sub>n</sub>}

Sample mean: 1/n sigma y<sub>i</sub>  
Properties: sigma (y<sub>i</sub> - mean) = 0

Also, affine transformation preserves linear combinations (i.e. the arithmetic mean changes accordingly by the affine transformation)

__Median__: the middle most observations  
arrange the data set in ascending order and pick the middle one. e.g. 1, 3, 7, 13, 25. Median is 7.

Median is not too sensitive with extreme values.

__Quartiles__: Q1: lower quartile, Q3: upper quartile

__Percentiles__: Instead of dividing into 4 parts, the data is divided into 100 parts.

Eg. p=0.25, m = (n+1) * p, n is the number of observations

__Mode__: Observation that occurs with the maximum frequency

St.Petersburg's Paradox. We care about averages (expected values), but also risk. Variability is a very important property of a data set. E.g. Country A 0 0 0 1000 and Country B 250 250 250 250
Goalie A 0 6 0 6 0 6 0 6 Goalie B 3 3 3 3 3 3 3 

Measures of Volatility
===

__Range__: Max - Min

__Interquartile Range (IQR)__: IQR = Q3 - Q1

__Sample Variance and Sample Standard Deviation__: 

__Sample Variance__: s<sup>2</sup> = 1/(n-1) sigma (y<sub>i</sub> - y average)<sup>2</sup> is approximatey the average of the squared deviation from the mean

__Standard Deviation__: s = positive square root of s^2, the sample variance

__Mean Absolute Deviation(MAD)__: 1/n sigma abs(y<sub>i</sub> - y average)

Say a data set of x<sub>i</sub> is transformed by an affine transformation y<sub>i</sub> = a + bx<sub>i</sub>. Then s<sup>2</sup> of y = b<sup>2</sup>s<sup>2</sup>. And the standard deviation is multiplied by the absolute value of b.

<br />

May 9, 2016 - Lecture 3
---

+ Practice problems with solutions on LEARN
+ <= Wednesday's class for the TQ1 next week

**Today**

+ Measures of Symmetry  
+ Measures of Kurtosis  
+ Applications  
+ Measures of Association  
+ Graphical Measures  

Objective: To figure out the shape of our sample

+ Centre: sample mean  
+ Variability: This measures how much the data set is spread out across the centre; how volatile the data set is; Use s<sup>2</sup>, variance; Standard deviation is the sqrt(variance)  
    + If y=a+bx, s<sup>2</sup><sub>y</sub> = b^2s^2_x s_y = abs(b)s_x  
    + The variance treats data on either side of the mean symmetrically
+ Symmetry:
    + 3 shapes:
        + Symmetric  
        + Right-skewed (mode is on the left side, mean is on the right side), long right tail
        + Left-skewed (mode is on the right side, mean is on the left side), long left tail
    + Measure of Skewness:
        + Quick estimate of skewness:  
            e.g. 1, 3, 5, 7, 9, 11, 13  
            Symmetric Data -> Mean = Median  
            e.g. 1, 3, 5, 7, 9, 11, 10<sup>7</sup>
            Mean >> Median  
            Measure of Skewness = Mean - Median    
            + if > 0, right-skewed, mean greater than median  
            + if < 0, left-skewed, mean less than median  
            + if = 0, symmetric
            
        
**Applications**  
Gould: "The median is not the message"  
"Abdominal mesothalamia": median of life expectancy after diagnosis = 8 months  

If the distribution (of frequency of death against time since diagnosis) is symmetric, then he will die within 16 months at best.

The actual distribution is right-skewed.

**Kurtosis**  
Kurtosis measure how frequent extreme observations are, with respect to the Normal Distribution.

Kurtosis checks whether the data set has "fatter" tails compared to the Normal distribution. 

Kurtosis: how much "peakedness" the graph has

For a normal distribution, kurtosis = 3. If k for your data > 3, then it has fatter tails compared to the normal. If it is < 3, then it has narrower tails.

Kurtosis checks whether we can apply the Normal Distribution assumption to our data set by comparing our sample kurtosis to 3.

**Measures of Association**  
Objective: To find whether there is any evidence of association between two variables

+ Categorical Variables
    <tr><td>lmao</td>
        <td>ayy</td></tr>
    <tr><td>hey</td></tr>
    + **Relative Risk**: measures the association between two catgorical variables; Measured by [R.R.]
    + A and B are indepedent iff P(A\|B) = P(A)  
    + If R.R. is approximately equal to 1, then there is little evidence of association
    + If R.R. > > 1 or < < 1, evidence of association
    + But how high is too high? UNANSWERED QUESTION
    + The table = Two Way Contingency Table
    
+ Numerical Variables  
How to find a measure of association between two numerical variables?
    + Sample Correlation Coefficient r<sub>xy</sub>: the sign of r tells us the direction of the relationship, and the value of r tells us the strength of the association/relationship
    

E.g.   
x<sub>i</sub> = # of beers you drink  
y<sub>i</sub> = STAT 231 mark  
Collect a data set:  
{x1, y1},...{xn, yn}

So it is most likely that if xn - x average > 0, yn - y average < 0 

Thus, the numerator tells us the direction of the relationship. The denominator ensures that -1 <= r<sub>xy</sub> <= 1

The closer abs(r_xy) is to 1, the strong is the evidence of association.

r captures the **linear relationship** between x and y. Not good in capture non-linear relationship

E.g. y = x<sup>2</sup>, x=-3, y=9, x=-1, y=1, x=1, y=1, x=3, y=9.  r_xy = 0 !!! It's a function so the two variables are associated for sure, but the Coefficient gives 0. Does not work well with quadratic relationships, or non-linear in general.

E.g. y = ax + b. r_xy = 1 if a > 0, = -1 if a < 0

Since we are going from sample to population, we can only find evidence of association. Strong correlation != Causation

**Graphical Measures**  
+ Density Histogram: Histogram -> grouped data; Density histogram -> area of every rectangle = relative frequency of the group
+ Empirical cdf  
+ Box Plot  
+ Scatter Plot  
+ Q-Q Plot  

